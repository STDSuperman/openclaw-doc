---
title: "AI 模型优化"
category: "AI模型"
difficulty: "intermediate"
estimated_time: "15 min"
tags: ["模型", "优化", "token管理", "成本", "缓存"]
prerequisites: ["AI模型"]
related_docs: ["模型选择", "模型故障排查"]
next_steps: []
last_updated: "2026-01-01"
source: "docs/concepts/model-failover.md, docs/concepts/CONTEXT.md"
---


# AI 模型优化

## 令牌管理

### 认证配置文件

使用不同的认证配置文件来分离使用场景:
- 个人使用 vs 工作使用
- 开发者账户 vs 测试账户

### 会话固定

为常用会话固定特定模型:
```bash
# 在常用会话中固定到 Opus
openclaw agent --to +15551234567 --model opus
```

### 令牌冷却

避免快速故障转移:
- 配置合理的冷却时间(`auth.cooldownMs`)
- 监控使用统计
- 避免在短时间内频繁切换模型

## 成本控制

### 使用更便宜的模型

为不同任务选择合适成本的模型:
- 简单任务: 使用 GPT-4o-mini
- 复杂分析: 使用 Claude Opus-4-5

### 限制令牌使用

```json5
{
  agents: {
    defaults: {
      maxTokens: 100000,  // 限制每次请求的最大令牌数
    },
  },
}
```

### 使用统计

启用详细的使用统计:
```bash
openclaw models status --verbose
```

查看每消息的令牌使用:
```bash
/open usage
```

## 缓存策略

### 提供商缓存

OpenClaw 优先使用最近使用的认证配置文件:
- 保持提供商缓存热
- 减少认证开销
- 避免不必要的令牌刷新

### 上下文管理

### 减少上下文大小

使用 `/compact` 命令来减少历史:
```bash
openclaw sessions compact <session-id>
```

### 配置上下文限制

```json5
{
  agents: {
    defaults: {
      maxContextTokens: 200000,  // 限制上下文令牌数
      maxContextMessages: 100,  // 限制上下文消息数
    },
  },
}
```

## 性能优化

### 请求批处理

OpenClaw 会批量处理相关请求以提高效率:
- 自动批处理消息
- 智能调度工具调用
- 并行化独立任务

### 响应流式

使用流式响应以提高实时性:
- 减少首字延迟
- 提供渐进式更新

### 超时优化

根据网络情况调整超时:
- 高速网络: 降低超时
- 低速网络: 增加超时

## 使用统计优化

### 定期审查

定期检查使用情况:
- 令牌使用趋势
- 成本分析
- 错误模式识别
- 性能瓶颈

### 容量规划

根据使用模式规划容量:
- 预期峰值使用
- 多实例部署
- 负载均衡

## 高级技巧

### 智能模型选择

根据任务复杂度自动选择模型:
- 简单查询 → 快速模型
- 复杂推理 → 智能模型
- 创意任务 → 多模态模型

### 条件路由

根据内容路由到不同模型:
- 技术问题 → 专业模型
- 日常任务 → 标准模型
- 高优先级 → 最高性能模型

### 容错机制

- 自动重试失败的请求
- 智能降级到备用模型
- 优雅处理超时和错误

## 监控和告警

### 设置监控

监控关键指标:
- 成功率
- 响应时间
- 令牌使用
- 错误率

### 性能基准

建立性能基准:
- 记录典型任务的响应时间
- 定期对比性能
- 识别性能退化

## 最佳实践

1. **使用会话固定**: 为常用工作固定模型
2. **分层模型使用**: 为不同任务类型配置不同模型
3. **监控成本**: 定期审查使用情况
4. **优化缓存**: 利用提供商缓存
5. **批量操作**: 充分利用批处理能力
6. **流式响应**: 对长文本任务启用流式
7. **定期优化**: 根据使用模式调整配置
